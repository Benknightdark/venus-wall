{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 📝 安裝套件"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install jieba -U\r\n",
    "!pip install httpx -U\r\n",
    "!pip install sklearn -U\r\n",
    "!pip install ckiptagger[tfgpu,gdown] -U\r\n",
    "!pip install BeautifulSoup4 -U\r\n",
    "'''\r\n",
    "1. 抓資料\r\n",
    "2. 資料清洗\r\n",
    "3. 長詞優先斷詞+jieba斷詞+ckiptagger斷詞\r\n",
    "4. tf-idf取出關鍵字\r\n",
    "5. 計算文件相似度\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📝 針對標點符號斷行"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "import re\r\n",
    "def splitSentense(text, delimiter):\r\n",
    "    return re.split(delimiter, text)\r\n",
    "    \r\n",
    "delimiter = \"，|。|、|（|）|／|《|》|】|【|「|」|；|：\"\r\n",
    "allNews = ['虛擬貨幣「比特幣期貨」要來了！美國監管機構美國商品期貨交易委員會（CFTC）一日宣布將放行比特幣期貨，允許芝加哥商品交易所（CME）和芝加哥選擇權交易所（CBOE）推出相關期貨合約，因為兩交易所已證明擬推出的合約和交易安全符合必要的監管規定；這成為推動主流投資人買賣此價格高度波動的數位貨幣的重大一步。']\r\n",
    "sentenceAry = []\r\n",
    "for rec in allNews:\r\n",
    "    text = rec\r\n",
    "    sentenceAry += splitSentense(text,delimiter)\r\n",
    "sentenceAry    "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['虛擬貨幣',\n",
       " '比特幣期貨',\n",
       " '要來了！美國監管機構美國商品期貨交易委員會',\n",
       " 'CFTC',\n",
       " '一日宣布將放行比特幣期貨',\n",
       " '允許芝加哥商品交易所',\n",
       " 'CME',\n",
       " '和芝加哥選擇權交易所',\n",
       " 'CBOE',\n",
       " '推出相關期貨合約',\n",
       " '因為兩交易所已證明擬推出的合約和交易安全符合必要的監管規定',\n",
       " '這成為推動主流投資人買賣此價格高度波動的數位貨幣的重大一步',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📝 ngram + 長詞優先斷詞 => 取出特殊關鍵詞"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import operator\r\n",
    "def removeKey(text, keyword):\r\n",
    "    textAry= text\r\n",
    "    for key in keyword:\r\n",
    "        textAry = ''.join(textAry.split(key))\r\n",
    "    return textAry\r\n",
    "def ngram(input_sentence, n = 2):\r\n",
    "    word_dic = {}\r\n",
    "    sentence  = input_sentence\r\n",
    "    for i in range(0, len(sentence) - n + 1):        \r\n",
    "        if sentence[i:i+n] not in word_dic:\r\n",
    "            word_dic[sentence[i:i+n]] = 1\r\n",
    "        else:\r\n",
    "            word_dic[sentence[i:i+n]] = word_dic[sentence[i:i+n]] + 1\r\n",
    "    return word_dic    \r\n",
    "keywords=['期貨','核准']        \r\n",
    "ret_terms={}\r\n",
    "words_freq    = []\r\n",
    "for term_length in range(4,1,-1):\r\n",
    "    word_dic = {}\r\n",
    "    for sentence in sentenceAry:\r\n",
    "        text_list = removeKey(sentence,keywords)  \r\n",
    "        ngram_words = ngram(text_list,term_length) \r\n",
    "        for word in ngram_words:\r\n",
    "            if word not in word_dic:\r\n",
    "                word_dic[word] = 1\r\n",
    "            else:\r\n",
    "                word_dic[word] += ngram_words[word]   \r\n",
    "    for word in word_dic:\r\n",
    "        if word_dic[word] >= 5:\r\n",
    "            keywords.append(word)            \r\n",
    "            ret_terms.update({word:word_dic[word]})\r\n",
    "sorted_terms = sorted(ret_terms.items(),key=operator.itemgetter(1),reverse=True) \r\n",
    "sorted_terms\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('交易', 5)]"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📝 tf-idf"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "import jieba.analyse\r\n",
    "sentence = '''有外貌美麗如女星范冰冰的馮姓女子2008年與陳姓富二代訂婚後無故悔婚，但馮女不告而別2年後，忽然回頭請陳男金援，癡情的陳男不但不計前嫌，對馮女數十次索討生日、情人節等禮物及資助學費等一概答應，竟連她結婚生子後要求生活費，也大方匯給10萬元，直到陳男也結婚，妻發現他竟贈百萬名畫給馮女，陳男才在妻逼迫下告馮女詐欺、侵占，並請求返還借款533萬餘元，但檢方認定陳男自願贈與，將馮女不起訴，台北地院也僅認定2013年間馮女謊稱買房討「房貸頭期款」部分陳男求償有理，判馮女須賠60萬元。'''\r\n",
    "tags = jieba.analyse.extract_tags(sentence,3, withWeight=True)\r\n",
    "tags\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\smart\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.557 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('馮女', 0.6210268832675324),\n",
       " ('萬元', 0.3105134416337662),\n",
       " ('認定', 0.3105134416337662)]"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 📝 查詢相似文件"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import httpx\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import time\r\n",
    "import asyncio\r\n",
    "import csv\r\n",
    "import os\r\n",
    "import re\r\n",
    "\r\n",
    "headers = {'User-Agent': 'GoogleBot'}\r\n",
    "start = 1\r\n",
    "end = 50\r\n",
    "root_url = 'https://www.ithome.com.tw'\r\n",
    "url = f'{root_url}/devops'\r\n",
    "fetched_news_data=[]\r\n",
    "\r\n",
    "async def get_news(news_url, page):\r\n",
    "    req = httpx.get(f'{news_url}?page={page}', headers=headers)\r\n",
    "    res = req.text\r\n",
    "    root = BeautifulSoup(res, 'lxml')\r\n",
    "    if len(root.find_all('div',attrs={'class':'span4 channel-item'}))==0:\r\n",
    "        print(f'--------------沒有任何新聞內容了~~~~~-----------')\r\n",
    "        return\r\n",
    "    news_list = root.find_all(\r\n",
    "        'span', attrs={'class': 'views-field views-field-created'})    \r\n",
    "    for news in news_list:\r\n",
    "        news_div = news.span.div\r\n",
    "        categories = ','.join(list(map(lambda x: x.text, news_div.find(\r\n",
    "            'p', attrs={'class': 'category'}).find_all('a'))))\r\n",
    "        print(categories)\r\n",
    "        title_div = news_div.find('p', attrs={'class': 'title'})\r\n",
    "        title = title_div.a.text.strip()\r\n",
    "        href = f\"{root_url}{title_div.a['href']}\"\r\n",
    "        content_req=httpx.get(href)\r\n",
    "        content_res=content_req.text\r\n",
    "        content_root=BeautifulSoup(content_res, 'lxml')\r\n",
    "        content=content_root.find('div',attrs={'class':'field-type-text-with-summary'}).div.div.text\r\n",
    "        print(content)\r\n",
    "        id=title_div.a['href'].split('/')[2]\r\n",
    "        print(title)\r\n",
    "        print(href)\r\n",
    "        print(id)\r\n",
    "        summary = news_div.find('div', attrs={'class': 'summary'}).text.strip()\r\n",
    "        print(summary)\r\n",
    "        post_date = news_div.find('p', attrs={'class': 'post-at'}).text.strip()\r\n",
    "        print(post_date)\r\n",
    "        fetched_news_data.append([id,page,post_date,title,summary,content,href,categories])\r\n",
    "    print(f'--------------{page}-----------')\r\n",
    "\r\n",
    "\r\n",
    "async def main():\r\n",
    "    task_list = []\r\n",
    "    for i in range(start, end+1):\r\n",
    "        task_list.append(get_news(url, i))\r\n",
    "    await asyncio.gather(*task_list)\r\n",
    "\r\n",
    "s = time.perf_counter()\r\n",
    "await main()\r\n",
    "elapsed = time.perf_counter() - s\r\n",
    "\r\n",
    "if os.path.exists(\"output.csv\"):\r\n",
    "  os.remove(\"output.csv\")\r\n",
    "else:\r\n",
    "  print(\"The file does not exist\")\r\n",
    "\r\n",
    "with open('output.csv', 'w', newline='', encoding='UTF-8') as csvfile:\r\n",
    "  writer = csv.writer(csvfile)\r\n",
    "  writer.writerow(['ID','page', 'post_date', 'title','summary','content','href','categories'])\r\n",
    "  for data in fetched_news_data:\r\n",
    "    writer.writerow(data)\r\n",
    "print(f\"Script executed in {elapsed:0.2f} seconds.\")\r\n",
    "    \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pandas\r\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import jieba\r\n",
    "news = pandas.read_csv('output.csv')\r\n",
    "\r\n",
    "corpus = []\r\n",
    "titles = []\r\n",
    "for article in news.iterrows():\r\n",
    "    titles.append(article[1]['title'])\r\n",
    "    try:\r\n",
    "        corpus.append(' '.join(jieba.cut(article[1]['content'])))\r\n",
    "    except Exception as e:\r\n",
    "        print(article)\r\n",
    "        raise e   \r\n",
    "vectorizer = CountVectorizer() \r\n",
    "X = vectorizer.fit_transform(corpus)\r\n",
    "transformer = TfidfTransformer()\r\n",
    "tfidf = transformer.fit_transform(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\smart\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.558 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_similarity_sentence(articleid):\r\n",
    "    print('[查詢文章]:{}'.format(titles[articleid]))\r\n",
    "    cosine_similarities = cosine_similarity(tfidf[articleid], tfidf).flatten()\r\n",
    "    related_docs_indices = cosine_similarities.argsort()[-2::-1]\r\n",
    "    for idx in related_docs_indices:\r\n",
    "        if cosine_similarities[idx] > 0.3:\r\n",
    "            print('[相關文章]:{} {}'.format(titles[idx], cosine_similarities[idx]))\r\n",
    "get_similarity_sentence(100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[查詢文章]:GCP用戶現可在GKE上執行Cloud Dataproc的Spark工作負載\n",
      "[相關文章]:Google雲端Spark叢集服務Dataproc現可運用GPU加速運算 0.5262928890852002\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}