{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“ å®‰è£å¥—ä»¶"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install jieba -U\r\n",
    "!pip install httpx -U\r\n",
    "!pip install sklearn -U\r\n",
    "!pip install ckiptagger[tfgpu,gdown] -U\r\n",
    "!pip install BeautifulSoup4 -U\r\n",
    "'''\r\n",
    "1. æŠ“è³‡æ–™\r\n",
    "2. è³‡æ–™æ¸…æ´—\r\n",
    "3. é•·è©å„ªå…ˆæ–·è©+jiebaæ–·è©+ckiptaggeræ–·è©\r\n",
    "4. tf-idfå–å‡ºé—œéµå­—\r\n",
    "5. è¨ˆç®—æ–‡ä»¶ç›¸ä¼¼åº¦\r\n",
    "'''"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“ é‡å°æ¨™é»ç¬¦è™Ÿæ–·è¡Œ"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "import re\r\n",
    "def splitSentense(text, delimiter):\r\n",
    "    return re.split(delimiter, text)\r\n",
    "    \r\n",
    "delimiter = \"ï¼Œ|ã€‚|ã€|ï¼ˆ|ï¼‰|ï¼|ã€Š|ã€‹|ã€‘|ã€|ã€Œ|ã€|ï¼›|ï¼š\"\r\n",
    "allNews = ['è™›æ“¬è²¨å¹£ã€Œæ¯”ç‰¹å¹£æœŸè²¨ã€è¦ä¾†äº†ï¼ç¾åœ‹ç›£ç®¡æ©Ÿæ§‹ç¾åœ‹å•†å“æœŸè²¨äº¤æ˜“å§”å“¡æœƒï¼ˆCFTCï¼‰ä¸€æ—¥å®£å¸ƒå°‡æ”¾è¡Œæ¯”ç‰¹å¹£æœŸè²¨ï¼Œå…è¨±èŠåŠ å“¥å•†å“äº¤æ˜“æ‰€ï¼ˆCMEï¼‰å’ŒèŠåŠ å“¥é¸æ“‡æ¬Šäº¤æ˜“æ‰€ï¼ˆCBOEï¼‰æ¨å‡ºç›¸é—œæœŸè²¨åˆç´„ï¼Œå› ç‚ºå…©äº¤æ˜“æ‰€å·²è­‰æ˜æ“¬æ¨å‡ºçš„åˆç´„å’Œäº¤æ˜“å®‰å…¨ç¬¦åˆå¿…è¦çš„ç›£ç®¡è¦å®šï¼›é€™æˆç‚ºæ¨å‹•ä¸»æµæŠ•è³‡äººè²·è³£æ­¤åƒ¹æ ¼é«˜åº¦æ³¢å‹•çš„æ•¸ä½è²¨å¹£çš„é‡å¤§ä¸€æ­¥ã€‚']\r\n",
    "sentenceAry = []\r\n",
    "for rec in allNews:\r\n",
    "    text = rec\r\n",
    "    sentenceAry += splitSentense(text,delimiter)\r\n",
    "sentenceAry    "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['è™›æ“¬è²¨å¹£',\n",
       " 'æ¯”ç‰¹å¹£æœŸè²¨',\n",
       " 'è¦ä¾†äº†ï¼ç¾åœ‹ç›£ç®¡æ©Ÿæ§‹ç¾åœ‹å•†å“æœŸè²¨äº¤æ˜“å§”å“¡æœƒ',\n",
       " 'CFTC',\n",
       " 'ä¸€æ—¥å®£å¸ƒå°‡æ”¾è¡Œæ¯”ç‰¹å¹£æœŸè²¨',\n",
       " 'å…è¨±èŠåŠ å“¥å•†å“äº¤æ˜“æ‰€',\n",
       " 'CME',\n",
       " 'å’ŒèŠåŠ å“¥é¸æ“‡æ¬Šäº¤æ˜“æ‰€',\n",
       " 'CBOE',\n",
       " 'æ¨å‡ºç›¸é—œæœŸè²¨åˆç´„',\n",
       " 'å› ç‚ºå…©äº¤æ˜“æ‰€å·²è­‰æ˜æ“¬æ¨å‡ºçš„åˆç´„å’Œäº¤æ˜“å®‰å…¨ç¬¦åˆå¿…è¦çš„ç›£ç®¡è¦å®š',\n",
       " 'é€™æˆç‚ºæ¨å‹•ä¸»æµæŠ•è³‡äººè²·è³£æ­¤åƒ¹æ ¼é«˜åº¦æ³¢å‹•çš„æ•¸ä½è²¨å¹£çš„é‡å¤§ä¸€æ­¥',\n",
       " '']"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“ ngram + é•·è©å„ªå…ˆæ–·è© => å–å‡ºç‰¹æ®Šé—œéµè©"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "import operator\r\n",
    "def removeKey(text, keyword):\r\n",
    "    textAry= text\r\n",
    "    for key in keyword:\r\n",
    "        textAry = ''.join(textAry.split(key))\r\n",
    "    return textAry\r\n",
    "def ngram(input_sentence, n = 2):\r\n",
    "    word_dic = {}\r\n",
    "    sentence  = input_sentence\r\n",
    "    for i in range(0, len(sentence) - n + 1):        \r\n",
    "        if sentence[i:i+n] not in word_dic:\r\n",
    "            word_dic[sentence[i:i+n]] = 1\r\n",
    "        else:\r\n",
    "            word_dic[sentence[i:i+n]] = word_dic[sentence[i:i+n]] + 1\r\n",
    "    return word_dic    \r\n",
    "keywords=['æœŸè²¨','æ ¸å‡†']        \r\n",
    "ret_terms={}\r\n",
    "words_freq    = []\r\n",
    "for term_length in range(4,1,-1):\r\n",
    "    word_dic = {}\r\n",
    "    for sentence in sentenceAry:\r\n",
    "        text_list = removeKey(sentence,keywords)  \r\n",
    "        ngram_words = ngram(text_list,term_length) \r\n",
    "        for word in ngram_words:\r\n",
    "            if word not in word_dic:\r\n",
    "                word_dic[word] = 1\r\n",
    "            else:\r\n",
    "                word_dic[word] += ngram_words[word]   \r\n",
    "    for word in word_dic:\r\n",
    "        if word_dic[word] >= 5:\r\n",
    "            keywords.append(word)            \r\n",
    "            ret_terms.update({word:word_dic[word]})\r\n",
    "sorted_terms = sorted(ret_terms.items(),key=operator.itemgetter(1),reverse=True) \r\n",
    "sorted_terms\r\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('äº¤æ˜“', 5)]"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“ tf-idf"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "import jieba.analyse\r\n",
    "sentence = '''æœ‰å¤–è²Œç¾éº—å¦‚å¥³æ˜ŸèŒƒå†°å†°çš„é¦®å§“å¥³å­2008å¹´èˆ‡é™³å§“å¯ŒäºŒä»£è¨‚å©šå¾Œç„¡æ•…æ‚”å©šï¼Œä½†é¦®å¥³ä¸å‘Šè€Œåˆ¥2å¹´å¾Œï¼Œå¿½ç„¶å›é ­è«‹é™³ç”·é‡‘æ´ï¼Œç™¡æƒ…çš„é™³ç”·ä¸ä½†ä¸è¨ˆå‰å«Œï¼Œå°é¦®å¥³æ•¸åæ¬¡ç´¢è¨ç”Ÿæ—¥ã€æƒ…äººç¯€ç­‰ç¦®ç‰©åŠè³‡åŠ©å­¸è²»ç­‰ä¸€æ¦‚ç­”æ‡‰ï¼Œç«Ÿé€£å¥¹çµå©šç”Ÿå­å¾Œè¦æ±‚ç”Ÿæ´»è²»ï¼Œä¹Ÿå¤§æ–¹åŒ¯çµ¦10è¬å…ƒï¼Œç›´åˆ°é™³ç”·ä¹Ÿçµå©šï¼Œå¦»ç™¼ç¾ä»–ç«Ÿè´ˆç™¾è¬åç•«çµ¦é¦®å¥³ï¼Œé™³ç”·æ‰åœ¨å¦»é€¼è¿«ä¸‹å‘Šé¦®å¥³è©æ¬ºã€ä¾µå ï¼Œä¸¦è«‹æ±‚è¿”é‚„å€Ÿæ¬¾533è¬é¤˜å…ƒï¼Œä½†æª¢æ–¹èªå®šé™³ç”·è‡ªé¡˜è´ˆèˆ‡ï¼Œå°‡é¦®å¥³ä¸èµ·è¨´ï¼Œå°åŒ—åœ°é™¢ä¹Ÿåƒ…èªå®š2013å¹´é–“é¦®å¥³è¬Šç¨±è²·æˆ¿è¨ã€Œæˆ¿è²¸é ­æœŸæ¬¾ã€éƒ¨åˆ†é™³ç”·æ±‚å„Ÿæœ‰ç†ï¼Œåˆ¤é¦®å¥³é ˆè³ 60è¬å…ƒã€‚'''\r\n",
    "tags = jieba.analyse.extract_tags(sentence,3, withWeight=True)\r\n",
    "tags\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\smart\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.557 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('é¦®å¥³', 0.6210268832675324),\n",
       " ('è¬å…ƒ', 0.3105134416337662),\n",
       " ('èªå®š', 0.3105134416337662)]"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ğŸ“ æŸ¥è©¢ç›¸ä¼¼æ–‡ä»¶"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import httpx\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "import time\r\n",
    "import asyncio\r\n",
    "import csv\r\n",
    "import os\r\n",
    "import re\r\n",
    "\r\n",
    "headers = {'User-Agent': 'GoogleBot'}\r\n",
    "start = 1\r\n",
    "end = 50\r\n",
    "root_url = 'https://www.ithome.com.tw'\r\n",
    "url = f'{root_url}/devops'\r\n",
    "fetched_news_data=[]\r\n",
    "\r\n",
    "async def get_news(news_url, page):\r\n",
    "    req = httpx.get(f'{news_url}?page={page}', headers=headers)\r\n",
    "    res = req.text\r\n",
    "    root = BeautifulSoup(res, 'lxml')\r\n",
    "    if len(root.find_all('div',attrs={'class':'span4 channel-item'}))==0:\r\n",
    "        print(f'--------------æ²’æœ‰ä»»ä½•æ–°èå…§å®¹äº†~~~~~-----------')\r\n",
    "        return\r\n",
    "    news_list = root.find_all(\r\n",
    "        'span', attrs={'class': 'views-field views-field-created'})    \r\n",
    "    for news in news_list:\r\n",
    "        news_div = news.span.div\r\n",
    "        categories = ','.join(list(map(lambda x: x.text, news_div.find(\r\n",
    "            'p', attrs={'class': 'category'}).find_all('a'))))\r\n",
    "        print(categories)\r\n",
    "        title_div = news_div.find('p', attrs={'class': 'title'})\r\n",
    "        title = title_div.a.text.strip()\r\n",
    "        href = f\"{root_url}{title_div.a['href']}\"\r\n",
    "        content_req=httpx.get(href)\r\n",
    "        content_res=content_req.text\r\n",
    "        content_root=BeautifulSoup(content_res, 'lxml')\r\n",
    "        content=content_root.find('div',attrs={'class':'field-type-text-with-summary'}).div.div.text\r\n",
    "        print(content)\r\n",
    "        id=title_div.a['href'].split('/')[2]\r\n",
    "        print(title)\r\n",
    "        print(href)\r\n",
    "        print(id)\r\n",
    "        summary = news_div.find('div', attrs={'class': 'summary'}).text.strip()\r\n",
    "        print(summary)\r\n",
    "        post_date = news_div.find('p', attrs={'class': 'post-at'}).text.strip()\r\n",
    "        print(post_date)\r\n",
    "        fetched_news_data.append([id,page,post_date,title,summary,content,href,categories])\r\n",
    "    print(f'--------------{page}-----------')\r\n",
    "\r\n",
    "\r\n",
    "async def main():\r\n",
    "    task_list = []\r\n",
    "    for i in range(start, end+1):\r\n",
    "        task_list.append(get_news(url, i))\r\n",
    "    await asyncio.gather(*task_list)\r\n",
    "\r\n",
    "s = time.perf_counter()\r\n",
    "await main()\r\n",
    "elapsed = time.perf_counter() - s\r\n",
    "\r\n",
    "if os.path.exists(\"output.csv\"):\r\n",
    "  os.remove(\"output.csv\")\r\n",
    "else:\r\n",
    "  print(\"The file does not exist\")\r\n",
    "\r\n",
    "with open('output.csv', 'w', newline='', encoding='UTF-8') as csvfile:\r\n",
    "  writer = csv.writer(csvfile)\r\n",
    "  writer.writerow(['ID','page', 'post_date', 'title','summary','content','href','categories'])\r\n",
    "  for data in fetched_news_data:\r\n",
    "    writer.writerow(data)\r\n",
    "print(f\"Script executed in {elapsed:0.2f} seconds.\")\r\n",
    "    \r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import pandas\r\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "import jieba\r\n",
    "news = pandas.read_csv('output.csv')\r\n",
    "\r\n",
    "corpus = []\r\n",
    "titles = []\r\n",
    "for article in news.iterrows():\r\n",
    "    titles.append(article[1]['title'])\r\n",
    "    try:\r\n",
    "        corpus.append(' '.join(jieba.cut(article[1]['content'])))\r\n",
    "    except Exception as e:\r\n",
    "        print(article)\r\n",
    "        raise e   \r\n",
    "vectorizer = CountVectorizer() \r\n",
    "X = vectorizer.fit_transform(corpus)\r\n",
    "transformer = TfidfTransformer()\r\n",
    "tfidf = transformer.fit_transform(X)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\smart\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.558 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def get_similarity_sentence(articleid):\r\n",
    "    print('[æŸ¥è©¢æ–‡ç« ]:{}'.format(titles[articleid]))\r\n",
    "    cosine_similarities = cosine_similarity(tfidf[articleid], tfidf).flatten()\r\n",
    "    related_docs_indices = cosine_similarities.argsort()[-2::-1]\r\n",
    "    for idx in related_docs_indices:\r\n",
    "        if cosine_similarities[idx] > 0.3:\r\n",
    "            print('[ç›¸é—œæ–‡ç« ]:{} {}'.format(titles[idx], cosine_similarities[idx]))\r\n",
    "get_similarity_sentence(100)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[æŸ¥è©¢æ–‡ç« ]:GCPç”¨æˆ¶ç¾å¯åœ¨GKEä¸ŠåŸ·è¡ŒCloud Dataprocçš„Sparkå·¥ä½œè² è¼‰\n",
      "[ç›¸é—œæ–‡ç« ]:Googleé›²ç«¯Sparkå¢é›†æœå‹™Dataprocç¾å¯é‹ç”¨GPUåŠ é€Ÿé‹ç®— 0.5262928890852002\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit"
  },
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}